---
title: "SCIE4002: Experimental Design and Data Analysis"
subtitle: "L05 - Generalized linear models - Linear, logistic and Poisson regression"
author: "John Ormerod"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  xaringan::moon_reader:
    css: ["default", "assets/sydney-fonts.css", "assets/sydney.css"]
    self_contained: true # if true, fonts will be stored locally
    seal: true # show a title slide with YAML information
    includes:
      in_header: ["assets/mathjax-config.html"]
    header-includes:
      - \usepackage{color}
    nature:
      beforeInit: ["assets/remark-zoom.js", "./assets/widgets.js"]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9' # alternatives '16:9' or '4:3' or others e.g. 13:9
      navigation:
        scroll: false # disable slide transitions by scrolling
 
---


```{r Lec1, echo=FALSE, warning=FALSE, message=FALSE, cache=FALSE}
set.seed(1)
```

### Outline

+ Simple Linear regression (revision).

+ Multiple Linear regression (revision - hopefully).

+ Relationship between linear regression with ANOVA and variants.

+ Logistic regression.
 
+ Poisson regression.

---

class: segue


.white[

# Simple Linear regression  

]


---

### Linear regression (revision)

In terms of models so far we have covered

+ t-tests: One continuous response variable + binary group variable.

+ One way ANOVA: One continuous response variable + group variable with multiple categories.
 
+ Two way ANOVA: One continuous response variable + two group variable with multiple categories and possible interactions.

All of these are special cases of (simple) linear regression or multiple linear regression.

Later we will show this.

---

### Simple linear regression - Motivating data

Consider the following dataset.

```{r}
dat <- read.csv("data/heightWeight.csv")
head(dat)
```

This dataset contains the variables height (continuous), weight (continuous), diet (binary - meat or veg),
and age (continuous).

Let's focus on the relationship between height and weight.

---

### Simple Linear regression

.pull-left[ 

Let's plot a scatterplot of 

+ x=height versus 

+ y=weight.

Here we are looking the relationship between two continuous variables.

Looking at the scatterplot one might hypothesize a linear relationship between height
and weight.

]

.pull-right[

```{r}
plot(dat$height, dat$weight, 
     xlab="height", ylab="weight")
```

]

---

### Simple Linear regression

A linear relationship looks like a good initial model for our data.



```{r, fig.height=5}
plot(dat$height, dat$weight, 
     xlab="height", ylab="weight")
mod <- lm(weight~height, data=dat)
abline(mod, col="red", lwd=2)
```

---

### Simple Linear regression

The equation of the fitted line is

$$y_i = \beta_0 + \beta_1 x_i + \varepsilon_i, \quad i=1,\ldots,n,$$

where $\varepsilon_i \sim N(0,\sigma^2)$ is random noise.

For our example 

+ The $y_i$'s correspond to each individual's weight in the dataset.

+ The $x_i$'s correspond to each individual's height in the dataset.

+ $\beta_0$ is the population intercept parameter.

+ $\beta_1$ is the population slope parameter.

Since the mean of $\varepsilon_i$ is zero the equation

$$y_i = \beta_0 + \beta_1 x_i$$

gives the predicted value of $y_i$ for a given $x_i$.

---

### Simple Linear regression - Assumptions


Linearity: 

The relationship between $y_i$ and $x_i$ is linear 

$$y_i = \beta_0 + \beta_1 x_i$$

An alternative model might be

$$y_i = \beta_0 + \beta_1 x_i^2$$
or 

$$y_i = \beta_0 + \beta_1 e^{x_i}$$

in which case the relationship between $y_i$ and $x_i$ is not linear.

---

### Simple Linear regression - Assumptions

Homoscedasticity or constant variance: 

+ The value of $\sigma^2$ does not depend on the value of $x_i$
(or any other predictor in our dataset).

.pull-left[
<img src="images/Homoscedasticity.png" style="width: 80%; align: center"/> </a>
]


.pull-right[
<img src="images/220px-Heteroscedasticity.png" style="width: 80%; align: center"/> </a>
]

When we looked at ANOVA we were assuming constant variance for each group.



---

### Simple Linear regression - Other assumptions

Some other assumptions are:

+ Independence: Each point pair $[x_i,y_i]$ do not depend on $\varepsilon_i$ or any other point pair.

+ Normality: The errors $\varepsilon_1,\ldots,\varepsilon_n$ follow a normal distribution.

Note: We do not need to assume that the $x_i$'s are normally distributed.

When these assumptions do not hold then our inferences might not be correct.

---

### Simple Linear regression - Assumptions - R code

```{r, fig.height=5}
mod <- lm(weight~height, data=dat)
par(mfrow=c(2,2))
plot(mod)
```

There appears to be a possible quadratic trend (the red line). Apart from a few outliers
the QQ-plot is almost perfect.


---

### Simple Linear regression - Fitting the model

If we have mean zero errors, independence, linearity, and homoscedasticity
(not necessariy normality!) then estimating the coefficients $\beta_0$
and $\beta_1$ using least squares is "best" (Gauss-Markov therem).

We choose $\beta_0$ and $\beta_1$ to minimize

$$\frac{1}{n}\sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)^2$$

In this course we won't concern ourselves about how this calculation is done
(but can be done using high school calculus).

---

### Simple Linear regression - Fitted values


```{r}
mod <- lm(weight~height, data=dat)
mod
```

From the above R code we see that the fitted values for the intercept and slope are

+ $\widehat{\beta}_0 = -82.2887$ and

+ $\widehat{\beta}_1 = 0.9786$ respectively.

We have placed "hats" on the parameters to indicate that these are fitted values
from the sample (and are no longer population parameters).


---

### Simple Linear regression - Fitted model and intepretation

The fitted model is

$$\widehat{y}_i = \widehat{\beta}_0 + \widehat{\beta}_1 x_i = -82.2887 +  0.9786 \times x_i$$
+ For every unit increase in $x_i$ the $y_i$ values increase by $0.9786$ kilograms.

+ When $x_i = 0$ (zero height - no one is this tall) then the predicted $y_i$ value
is $-82.2887$ kilograms (no one has the weight).

This highlights the dangers of extrapolating outside the range of our data.

---

### Simple Linear regression - Extrapoation

We should only use this model inside the range of the observed data. Otherwise
we can make non-sensical predictions - this is called extrapolation.

```{r, fig.height=5}
plot(dat$height, dat$weight, 
     xlab="height", ylab="weight",
     xlim=c(0,max(dat$height)),ylim=c(0,max(dat$weight)))
mod <- lm(weight~height, data=dat)
abline(mod, col="red", lwd=2)
```

---

### Simple Linear regression - Inference

```{r, echo=TRUE}
mod <- lm(weight~height, data=dat)
summary(mod)
```



---

### Simple Linear regression - Inference


+ The 5 number summary statistics for the residuals $r_i = y_i - \widehat{y}_i$ 
$i=1,\ldots,n$ are

| Min    | 1st quartile | Median | 3rd Quartile | Max    |
|--------|--------------|--------|--------------|--------|
| -46.777| 7.734        | -0.055 | 9.203        | 30.123 |   

+ The median being close to 0 indicates the data are not very skewed.

+ $\widehat{\beta}_0 = -82.2887$ and $\widehat{\beta}_1 = 0.9786$

+ $\mbox{se}(\widehat{\beta}_0) = 23.9323$ and $\mbox{se}(\widehat{\beta}_1) = 0.1358$

+ T-values are t-statisitcs: $\widehat{\beta}_j/\mbox{se}(\widehat{\beta}_j)$.

+ Hypothesis testing $H_0:\beta_0=0$ the p-value is 0.00122 - $\beta_0$ is significantly different from 0.

+ Hypothesis testing $H_0:\beta_1=0$ the p-value is $3.55 \times 10^{-9}$ - $\beta_1$ is significantly different from 0.

+ The fitted value of $\widehat{\sigma}$ is 14.45.

+ F-statistic is 51.94 (testing that the intercept model is adequate) has p-value
$3.55 \times 10^{-9}$ suggesting we reject the model containing the intercept only
in favor of the model with an intercept and slope.


---

### Simple Linear regression - R-squared


+ Let the Residual sum of squares for the full model be

$$\mbox{RSS}_1 = \frac{1}{n}\sum_{i=1}^n (y_i - \widehat{\beta}_0 - \widehat{\beta}_1 x_i)^2$$

+ Let the Residual sum of squares for the intercept model be

$$\mbox{RSS}_0 = \frac{1}{n}\sum_{i=1}^n (y_i - \widehat{\beta}_0)^2$$

+ Then the R-squared value is

$$R^2 = \frac{\mbox{RSS}_0  - \mbox{RSS}_1}{\mbox{RSS}_0}$$

+ The R-squared value is between 0 and 1 and can be interpreted as the amount of variability
explained by the linear regression model and is a measure of goodness of fit.

+ The R-squared value for this model is 0.5197

 
 

---

class: segue


.white[

# Multiple Linear regression  

]


---

### Multiple Linear regression - Motivating data

Let's consider again the same dataset we considered when looking at simple linear regression.

```{r}
dat <- read.csv("data/heightWeight.csv")
head(dat)
```

If we want to predict weight, we have two more predictors that we could add to our regression model.
Namely, diet and age. Adding these predictors into our model could improve our predictions.

---

### Multiple Linear regression - The Model

When going for simple linear regression to multiple linear regression we can add
additional predictors to our model. Our model becomes

$$y_i = \beta_0 + \beta_1 x_{i1} + \ldots + \beta_p x_{ip} + \varepsilon_i$$

where $\varepsilon_i \sim N(0,\sigma^2)$, $i=1,\ldots,n$.

Instead of having two coefficients and one variance parameter to estimate we
now have $p+2$ parameters where $p$ is the number of predictors.

For example 

+ $x_{i1}$ could be height
+ $x_{i2}$ could be diet (equal to 1)


---

### Multiple Linear regression - Fitting the Model

.pull-left[
Suppose that we add diet to our model

```{r, echo=TRUE}
mod <- lm(weight~height+diet, data=dat)
mod
```
]

.pull-right[
Since diet is a factor with two categories, the model that R fits is

$$y_i = \beta_0 + \beta_1 {\bf x}_{i1} + \beta_2 {\bf x}_{i2} + \varepsilon_i$$

where

+ $y_i$ corresponds to the $i$th person's weight.

+ $x_{i1}$ corresponds to the $i$th person's height.

+ $x_{i2}$ is 0 if $i$th person's diet is "meat" and 1 if the $i$th persons diet is "veg".

]

---

### Multiple Linear regression - Intepretting our model

The fitted model from our previous slide is

$$\mbox{weight}_i = -72.036 + 0.942 \times \mbox{height}_i - 7.622\times I(\mbox{diet}_i=veg)$$
The interpretation is

+ On average, for every 1 unit increase in height we would predict weight to increase by 0.942kg

+ On average, if the person's diet is "veg" we would predict their weight to be 7.622 less than if their diet was "meat" (which is the baseline).

---

### Multiple Linear regression - Plotting the Model

.pull-left[

```{r, echo=TRUE, eval=FALSE}
N <- 100
x <- seq(min(dat$height),
  max(dat$height),
  length=N)
y1 <- predict(mod,
  data.frame(height=x,diet="veg"))
y2 <- predict(mod,
  data.frame(height=x,diet="meat"))
col1 <- "red"
col2 <- "blue"
plot(dat$height,dat$weight,
     col=ifelse(dat$diet=="meat",col1,col2),
     xlab="weight", ylab="height")
lines(x,y1,col=col2,lwd=2)
lines(x,y2,col=col1,lwd=2)
```

]

.pull-right[
```{r, echo=FALSE, eval=TRUE}
N <- 100
x <- seq(min(dat$height),max(dat$height),length=N)
y1 <- predict(mod,data.frame(height=x,diet="veg"))
y2 <- predict(mod,data.frame(height=x,diet="meat"))
plot(dat$height,dat$weight,
     col=ifelse(dat$diet=="meat","red","blue"),
     xlab="weight", ylab="height")
lines(x,y1,col="blue",lwd=2)
lines(x,y2,col="red",lwd=2)
```
]

---

### Multiple linear regression - 2 continuous and 1 binary predictor

Next we are going to add "age" to our model.

```{r, echo=TRUE}
mod <- lm(weight~height+diet+age, data=dat)
mod
```


The fitted model from our previous slide is

$$\mbox{weight}_i = -72.036 + 0.8948 \times \mbox{height}_i -8.2640\times I(\mbox{diet}_i=veg) -0.1298 \times \mbox{age}_i$$
With a similar interpretation as before.

However, one might ask the question: Does height or age play a bigger role in determining weight?

---

### Multiple linear regression - 2 continuous and 1 binary predictor

The question on the previous slide is difficult to answer because
height and age are on different scales.

For this reason it is common to standardize continuous variables by their $z$-scores.

A z-score is given by

$$z = \frac{x_i - \mu}{\sigma}$$
where $\mu$ and $\sigma$ are estimated by their sample mean and sample standard deviation respectively.

If we perform this transformation on height and age we are able to compare their relative contributions because they will be on the same scale.

---

### Multiple linear regression - 2 continuous and 1 binary predictor


```{r, echo=TRUE}
dat$height <- scale(dat$height)
dat$age <- scale(dat$age)
mod <- lm(weight~height+diet+age, data=dat)
mod
```
We can now see that height plays a much more important role than age in determining
a person's weight.


---

### Multiple linear regression - 2 continuous and 1 binary predictor

Looking at the model summary...

```{r}
summary(mod)
```

we see that the coefficient for height is significantly different from 0
while the coefficient for age is not. 


---

class: segue


.white[

# From continuous response to non-continuous response

]


---

### Categorization of linear regression

The table below summarises the types of models that we have considered.


| Name                        | R Formula          | Comments                            |
|-----------------------------|--------------------|-------------------------------------|
| t-test                      | lm(y~x)            | y continuous, x binary              |
| 1-way ANOVA                 | lm(y~x)            | y continuous, x factor              |
| 2-way ANOVA                 | lm(y~x1*x2)        | y continuous, x1 & x2 factors       |
| simple linear regression    | lm(y~x)            | y continuous, x continuous          |
| multiple linear regression  | lm(y~x1+x2+x3)     | y continuous, x1,x2 & x3 continuous |


In the first three weeks we have covered the first three model types.


---

### Non-continuous responses

+ Often the response variable cannot take any continuous value.

+ The response could be

  - Positive continuous, e.g., height, blood pressure, time to respond to treatment.
  - Counts: Number of seizures, number of people in a household.
  - Binary: Did the treatment work? Does the person have cancer?
  - Ordinal: How did you rate the movie out of 5 stars?
  - Categorical: Eye colour, book genre.
  
+ In your projects what is the variable of interest?

---

### Models for non-continuous responses

We use different distributions to model different response types:

- Binary: Bernoulli.

- Counts: Poisson, Negative-Binomial.

- Positive continuous: Gamma, inverse-gamma, log-normal distributions.

- Ordinal: Beyond the scope of this course.

- Categorical: Multinomial.

In this course we will only focus on the first two cases, i.e., Bernoulli and Poisson.



---

### Bernoulli distribution

The Bernoulli distribution has one parameter $\rho\in[0,1]$ so that the probability
that the response $Y$ takes a binary value $y=0$ or $y=1$ is

\begin{align}
P(Y = y) = \rho^y (1-\rho)^{1-y}
\end{align}

So for example when $\rho = 0.7$

\begin{align}
P(Y = 1) & = & \rho^1 (1 - \rho)^{1 - 1} & = & \rho     & = & 0.7 \\
P(Y = 0) & = & \rho^0 (1 - \rho)^{1 - 0} & = & 1 - \rho & = & 0.3 \\
\end{align}

Expected value or mean is 

$$\mathbb{E}(Y) = \sum_{y=0}^\infty y P(Y=y) = \rho$$ 

and the variance is

$$\mbox{Var}(Y) = \rho(1 - \rho)$$

---

### Bernoulli distribution
 
```{r dbinom, fig.keep='hide', echo=FALSE, eval=FALSE}
library(ggplot2)
rho <- 0.7
ggplot(transform(data.frame(x=c(0:1)), y=dbinom(x, 1, rho)), aes(x, y)) + 
  geom_bar(stat="identity") +
  theme_bw()
```
 
.pull-left[ 
A histogram of the probabilities when $\rho=0.7$ is below.

```{r, ref.label = 'dbinom', echo=FALSE, eval=TRUE, fig.height=5, fig.width=6}
```

]

.pull-right[ 


A sequence of $n=10$ Bernoulli distributed values with $\rho=0.7$ is given below
```{r}
n <- 10
rho <- 0.7
rbinom(n,1,rho)
```

Increasing $\rho$ increases the average proportion of 1's to 0's, and decreasing  $\rho$ decreases the average proportion of 1's to 0's. The probabilities can be calculated using
```{r}
dbinom(0:1,1,rho)
```

]


---

### Poisson distribution

The Poisson distribution has one parameter $\lambda>0$ so that the probability
that the response $Y$ takes the count value $y=0,1,2,\ldots$ is

\begin{align}
P(Y = k) = {\displaystyle {\frac {\lambda ^{k}e^{-\lambda }}{k!}}}
\end{align}

So for example when $\lambda = 3$

\begin{align}
P(Y = 0) & = {\displaystyle {\frac {3^{0}e^{-3 }}{0!}}} = e^{-3}  \approx 0.0498 \\
P(Y = 1) & = {\displaystyle {\frac {3^{1}e^{-3 }}{1!}}} = 3e^{-3} \approx 0.1494 \\
P(Y = 2) & = {\displaystyle {\frac {3^{2}e^{-3 }}{2!}}} = \frac{9e^{-3}}{2} \approx 0.2240 \\
\end{align}

Expected value or mean and variance are respectively

$$\mathbb{E}(Y) = \sum_{y=0}^\infty y P(Y=y) = \lambda \qquad \mbox{and} \qquad \mbox{Var}(Y) = \lambda.$$ 

 

---

### Poisson distribution
 
```{r dpois, fig.keep='hide', echo=FALSE, eval=FALSE}
lambda <- 3
ggplot(transform(data.frame(x=c(0:10)), y=dpois(x, lambda)), aes(x, y)) + 
  geom_bar(stat="identity") + 
  scale_x_continuous(breaks=0:10) +
  theme_bw()
```
 
.pull-left[ 
A histogram of the probabilities when $\lambda=3$ is below.

```{r, ref.label = 'dpois', echo=FALSE, eval=TRUE, fig.height=5, fig.width=6}
```

]


.pull-right[

A sequence of $n=10$ Poisson distributed values with $\lambda=3$ is given below
```{r}
n <- 10
lambda <- 3
rpois(n,lambda)
```

Increasing $\lambda$ means on average higher counts, and decreasing $\lambda$ means on average that the counts
are lower. The probabilities can be calculated using
```{r, linewidth=50}
round(dpois(0:10,lambda),4)
```

]
 
---

### Using linear models for non-continuous responses


.pull-left[
Two main problems can occur when using linear models to model non-continuous responses

+ The uncertainty (variance) associated with parameter estimates are not calculated correctly. Consequently,
the $p$-values from hypothesis test for each regression coefficient will not be correct
(and so not valid).

+ Extrapolating the linear outside the domain of the observed data can be problematic.

]

.pull-right[


<center>
<img src="images/xkcd.png" style="width: 80%">
</center>

]

---

class: segue

.white[
# Logistic regression
]

---

### Example 

.pull-left[

For the 23 space shuttle flights that occurred before the Challenger
disaster of 1986 the table below gives the temperature (in degrees Fahrenheit)
at the time of launch and a code of 0-1 where 1 denotes at least one O-ring
suffered thermal distress (TD).

Thermal distress of O-rings was blamed for the disaster.
Seven people died in the incident.

<center>
<img src="images/Challenger_flight_51-l_crew.jpg" style="width: 60%">
</center>



]


.pull-right[

Flight| Temp | TD | Flight | Temp | TD 
---|---|---|---|---|---
1|  66| 0 | 13 | 67 | 0 
2|  70| 1 | 14 | 53 | 1 
3|  69| 0 | 15 | 67 | 0 
4|  68| 0 | 16 | 75 | 0 
5|  67| 0 | 17 | 70 | 0 
6|  72| 0 | 18 | 81 | 0 
7|  73| 0 | 19 | 76 | 0 
8|  70| 0 | 20 | 79 | 0 
9|  57| 1 | 21 | 75 | 1 
10| 63| 1 | 22 | 76 | 0 
11| 70| 1 | 23 | 58 | 1 
12| 78| 0 |    |    | 

]

---

### R code 

We have two variables

+ $y_i = \mbox{TD}_i$ an indicator of thermal distress.

+ $x_i = \mbox{temp}_i$ the temperature (in degrees Fahrenheit).

We can enter the data into R via
```{r}
TD   <- c(0,1,0,0,0,0,0,0,1,1,1,0,0,1,0,0,0,0,0,0,1,0,1)
temp <- c(66,70,69,68,67,72,73,70,57,63,70,78,67,53,67,75,70,81,76,79,75,76,58)
dat  <- data.frame(TD,temp)
```
 
---

```{r}
res4 <- lm(TD~temp, data=dat)
summary(res4)
```

 
---

### Using linear regression with binary data

.pull-left[
```{r, eval=TRUE, echo=FALSE}
x    <- seq(min(temp),max(temp),,1000)
yhat <- res4$coef[1] + res4$coef[2]*x
plot(temp,
     TD,
     xlim=range(x),
     ylim=range(TD),
     xlab="temperature",
     ylab="thermal distress")
lines(x,yhat,col="red",lwd=3)
```
]

.pull-right[

The linear regression fit gives estimated probabilities of events being
less than 0 and greater than one for certain values of the predictor.

]

---

### From linear regression to logistic regression

.pull-left-2[
If we are dealing with $y_i\in\{0,1\}$ a Bernoulli model might be more appropriate than a linear model, say,

$$y_i|x_i \stackrel{ind}{\sim} \mbox{Bernoulli}(\rho_i)$$
where $p_i$ is some function of ${\bf x}_i$. Since $p_i\in[0,1]$
we want this function to map a value of ${\bf x}_i$ to the unit interval.
This most common choice is

$$\rho_i = \frac{\exp(\beta_0 +   \beta_1 x_{i})}{1 + \exp(\beta_0 +   \beta_1 x_{i})}$$

or equivalently

$$\log\left( \frac{\rho_i}{1 - \rho_i} \right) = \beta_0 +   \beta_1 x_{i}$$

The LHS is the log-odds.



]

.pull-right-1[

```{r,echo=FALSE, fig.height=4.5}
library(ggplot2)
x <- seq(-6,6,,1000)
y <- 1/(1 + exp(-x))
df <- data.frame(x=x,y=y)
g <- ggplot(df,aes(x=x,y=y)) +
  geom_line(size=2,col="blue") +
  ggtitle("logistic function") +
  theme_bw() +
  theme(
    plot.title = element_text(color = "red", 
                              size = 30, 
                              face = "bold")
  )
g
```

This leads to **logistic regression** since $\log\left( \frac{x}{1 - x} \right)$
is sometimes referred to as the logistic function.  

]

---

## Odds

The **odds** are an alternative way of quantifying the probability of an event.

For some event $E$,
$$\operatorname{odds}(E) = \frac{P(E)}{1-P(E)}.$$

If we are told the odds of $E$ are $a$ to $b$, then
$$\operatorname{odds}(E) = \frac{a}{b} = \frac{a/(a+b)}{b/(a+b)},$$
which implies $P(E) = a/(a+b)$.

**Odds** feature in logistic regression.

---

### Probability, Odds, and log-odds

Let's look at some examples for probability, odds, and log-odds
to get a feel for them.

| $P(X)$ | $P(X)/(1 - P(X))$ | $\log[P(X)/(1 - P(X)]$ |
|--------|-------------------|------------------------|
| 0.01   | 0.0101            | $-4.5951$              |
| 0.1    | 0.1111            | $-2.1972$              |
| 0.5    | 1                 | $0$                    |
| 0.9    | 9                 | $2.1972$               |
| 0.99   | 99                | $4.5951$               |




---

### Logistic regression

So instead of

```{r, eval=FALSE}
res4 <- lm(TD~temp, data=dat)
```

for linear models. For generalized linear models for binary data we use
almost identical syntax...

```{r, eval=FALSE}
res5 <- glm(TD~temp, data=dat, family=binomial)
```

noting that the Bernoulli distribution is a special case of the binomial distribution.

So we are fitting a model of the form

$$y_i \sim \mbox{Bernoulli}(p_i) \quad \mbox{with} \qquad \log\left( \frac{p_i}{1 - p_i}\right)= \beta_0 + \beta_1\times \mbox{temp}_i$$
---


```{r}
res5 <- glm(TD~temp, data=dat, family=binomial)
summary(res5)
```

 

---

### Logistic regression - The fitted model

The fitted model is

$$y_i \sim \mbox{Bernoulli}(\widehat{p}_i)$$


$$\log\left( \frac{\widehat{p}_i}{1 - \widehat{p}_i}\right)= \widehat{\beta}_0 + \widehat{\beta}_1\times \mbox{temp}_i$$
or equivalently

$$\widehat{p}_i = \frac{\exp(\widehat{\beta}_0 + \widehat{\beta}_1\times \mbox{temp}_i)}{1 + \exp(\widehat{\beta}_0 + \widehat{\beta}_1\times \mbox{temp}_i)}$$

where

+ $\widehat{\beta}_0 = `r res5$coef[1]`$ and

+ $\widehat{\beta}_1 = `r res5$coef[2]`$.


---

### Plot the fit

.pull-left[

```{r, plot_challenger, fig.show="hide", echo=TRUE, eval=FALSE}
x <- seq(min(temp),max(temp),,1000)
df <- data.frame(temp=x)
yhat <- predict(res5, 
                newdata=df, 
                type="response")

plot(temp,
     TD,
     xlim=range(x),
     ylim=range(TD),
     xlab="temperature",
     ylab="probability of thermal distress")
lines(x,yhat,col="red",lwd=3)
```
]

.pull-right[
```{r, ref.label="plot_challenger", echo=FALSE, eval=TRUE}
```
]




---

## Titanic survival  

 

Data on passengers on the RMS Titanic, excluding the crew and some individual identifier variables.


 
+   **pclass** a factor with levels `1st` `2nd` `3rd`
+   **survived** a factor with levels `died` `survived`
+   **sex** a factor with levels `female` `male`
+   **age** passenger age in years (or fractions of a year, for children), a numeric vector; age is missing for 263 of the passengers
+   **sibsp** number of siblings or spouses aboard, integer,  0 to 8
+   **parch** number of parents or children aboard, integer,  0 to 6
 
---

```{r, message=FALSE}
library(tidyverse)
# install.packages(vcdExtra)
data("Titanicp", package = "vcdExtra")
glimpse(Titanicp)
```
 
---

```{r, fig.height = 7}
Titanicp %>% group_by(survived, pclass) %>% count() %>%
  ggplot(aes(x = pclass, y = n, fill = survived)) + 
  geom_bar(stat = "identity", position = "fill") + theme_classic(base_size = 30)
```

---

 

```{r, fig.height = 7}
Titanicp %>% group_by(survived, sex) %>% count() %>%
  ggplot(aes(x = sex, y = n, fill = survived)) + 
  geom_bar(stat = "identity", position = "fill") + theme_classic(base_size = 30)
```

---

 
.pull-left[
```{r, fig.height = 6, warning = FALSE}
Titanicp %>% 
  ggplot() + 
  aes(x = age, y = survived) + 
  geom_point(size = 10, 
             alpha = 0.1) + 
  theme_classic(base_size = 40)
```
]

.pull-right[
```{r, fig.height =6, warning=FALSE}
Titanicp %>% 
  ggplot(aes(x = age, y = survived)) + 
  geom_point(size = 10, alpha = 0.1) + 
  facet_grid(~pclass)  + 
  theme_classic(base_size = 30)
```
]



---

## Logistic regression

-   A logistic regression model begins with,
$$y_i | \boldsymbol{x}_i \sim \operatorname{Bernoulli}\left(\frac{\exp( \beta_0 + \beta_1 x_{i1}+\ldots+\beta_p x_{ip})}{1+\exp(\beta_0 + \beta_1 x_{i1}+\ldots+\beta_p x_{ip})}\right).$$
-   If we had a new observation vector $(x_{i1},\ldots,x_{ip})$ and we knew the $(\beta_1,\ldots,\beta_p)$ vector, we could calculate the probability that the corresponding $Y=1$

$$P(Y=1 | \boldsymbol{x}) = \frac{\exp(\beta_0 + \beta_1 x_{i1}+\ldots+\beta_p x_{ip})}{1+\exp(\beta_0 + \beta_1 x_{i1}+\ldots+\beta_p x_{ip})}.$$
-   If this probability is greater than 0.5, we would make the prediction $\hat{Y}=1$, otherwise we would predict $\hat{Y}=0$.

 



---

## Modelling the titanic data  

-   Start by converting survival to 0/1 (numeric) variable

```{r}
x = Titanicp %>% mutate(survived = ifelse(survived == "survived", 1, 0))
glimpse(x)
```

-   We treat `survived` and `died` as successes and failures from a Bernoulli (binomial) distribution where the probability of success is given by a transformation of a linear model of the predictors.

---

## Fit a logistic regression model   

.small[
```{r}
glm1 = glm(survived ~ pclass + sex + age, family = binomial, data = x)
summary(glm1)
```
]

---

### Interpreting output - coefficients

<center>
<img src="images/glmCoefficientsCapture.jpg" style="width: 80%">
</center>

---

### Interpreting output - coefficient estimated values

<center>
<img src="images/glmEstimatesCapture.jpg" style="width: 80%">
</center>

---

### Interpreting output - coefficient standard errors

<center>
<img src="images/glmStdErrCapture.jpg" style="width: 80%">
</center>

---

## Checking for significance  

Before we start to interpret our model and make predictions, we might want to know if we can drop any of the variables from the model.
This is equivalent to testing  

$$H_0\colon\, \beta_j=0$$ 

against the alternative 

$$H_1\colon\, \beta_j\neq0$$

We test that $\beta_j=0$ if the esimated value for $\beta_j$, that is $\widehat{\beta}_j$
is large (in absolute) magnitude we say that $\beta_j$ is significantly different from 0.
Formally we do a test

$$
Z = \frac{\widehat{\beta}_j}{\mbox{SE}(\widehat{\beta}_j)} \stackrel{\mbox{approx}}{\sim} N(0,1)
$$

---

## Where we find the test statistic and p-value in the summary output.

.pull-left[

Test statistics will be approximately $N(0,1)$ distributed.

<center>
<img src="images/glmTestStatCapture.jpg" style="width: 80%">
</center>

]


.pull-right[

P-values: Convention is, if the p-value is below 5% then the coefficient
is significantly different from 0, i.e., reject $H_0$ in favour of $H_1$.


<center>
<img src="images/glmPvalueCapture.jpg" style="width: 80%">
</center>


]

---

## Write down the fitted model  

```{r}
glm1
```

$$\text{logit}(p) = 3.5 - 1.3\,\text{pclass2nd} - 2.3\,\text{pclass3rd} - 2.5\,\text{sexmale} - 0.03\,\text{Age}$$

---

## What's this logit function?

The **logit** function is our **link** from a linear combination of the predictors to the probability of the outcome being equal to 1.

$$\operatorname{logit}(p) = \log\left(\frac{p}{1-p}\right)$$

-   It's the log-odds!

-   Our estimated coefficients are therefore interpreted as changes in the **log-odds**.

-   I.e. we can write out fitted model as 

$$\log\left(\frac{p}{1-p}\right) = 3.5 - 1.3\,\text{pclass2nd} - 2.3\,\text{pclass3rd} - 2.5\,\text{sexmale} - 0.03\,\text{Age}$$

.footnote[
The logistic function (introduced earlier) is the inverse of the logit function.
]

---

## Interpreting our coefficients   

$$\log\left(\frac{p}{1-p}\right) = 3.5 - 1.3\,\text{pclass2nd} - 2.3\,\text{pclass3rd} - 2.5\,\text{sexmale} - 0.03\,\text{age}$$

-   **Intercept** the log-odds of survival for an individual travelling in 1st class who is female and aged zero years old.
-   Holding sex and age constant, the `pclass2nd` coefficient represents the **difference** in the log-odds between someone travelling in 1st class and someone travelling in 2nd class.  In this case, it's **negative**, so we're saying that your odds of survival were lower if you travelled in second class, relative to those who travelled in first class.
-   Holding class and age constant, the `sexmale` coefficient represents the **difference** in the log-odds between males and females.  It is **negative,** so we can say that if you were a male, your odds of survival were **lower** than if you were a female.
-   The `age` coefficient is also negative, which implies that older people had lower odds of survival than younger people.  Specifically, on average, for each additional year older you are, the log-odds of survival decreased by 0.03, holding class and sex constant.

---

## What do our predictions mean?  

$$\log\left(\frac{p}{1-p}\right) = 3.5 - 1.3\,\text{pclass2nd} - 2.3\,\text{pclass3rd} - 2.5\,\text{sexmale} - 0.03\,\text{Age}$$

We can predict the log-odds for a newborn male travelling in first class 

-   `pclass2nd = 0`, `pclass3rd = 0`, `sexmale = 1`, `age = 0`

$$\log\left(\frac{p}{1-p}\right) = 3.5 - 1.3\times 0 - 2.3\times 0 - 2.5\times 1 - 0.03\times 0 = 3.5 - 2.5 = 1$$

So the odds of survival for a newborn male travelling in first class are 1.

```{r}
new_data = data.frame(pclass = "1st", sex = "male", age = 0)
predict(glm1, newdata = new_data, type = "link")
```

---

  

Can we work out the estimated probability of survival for a newborn male travelling in first class?

$$\begin{aligned}
\log\left(\frac{p}{1-p}\right) & = 1 \\
\left(\frac{p}{1-p}\right) & = \exp(1) \\
p & = \exp(1) - p\exp(1) \\ 
p + p\exp(1) & = \exp(1) \\
p & = \frac{\exp(1)}{1+\exp(1)} \approx 0.73
\end{aligned}$$


```{r}
new_data = data.frame(pclass = "1st", sex = "male", age = 0)
predict(glm1, newdata = new_data, type = "response")
```

Note that we've used the **logistic** function to transform back to obtain an estimate of the **probability** (from the output of our model which is an estimate of the log-odds).

 
---

### Extensions to Logistic regression

+ Multinomial logistic regression: When the response variable is an unordered category. This type of model can be fit using the vglm() function in the VGAM package.

+ Ordinal logistic regression: When the response variable is an unordered category. This type of model can be fit using the polr() function in the MASS package.

\begin{align}
P(Y = k) = {\displaystyle {\frac{\lambda^{k}e^{-\lambda}}{k!}}}
\end{align}

The parameter $\lambda>0$ controls the magnitude of the counts.


For the normal distribution we let 

$$\mu_i = \beta_0 + \beta_1 x_{i1} + \ldots + \beta_p x_{ip}$$

Suppose that we use

\begin{align}
\lambda_i & = \beta_0 + \beta_1 x_{i1} + \ldots + \beta_p x_{ip}
\end{align}

This wont work because $\lambda$ is positive and the linear combination
$\beta_0 + \beta_1 x_{i1} + \ldots + \beta_p x_{ip}$ could be negative.

---
class: segue

.white[
# Poisson linear models
]

---

### Using linear regression with count data

In the following example we look at the number of awards earned by students at one high school, $y_i$
with $n=200$ students.
Predictors of the number of awards earned include the type of program in which the student was enrolled 
(e.g., vocational, general or academic) and the score on their final exam in math.

```{r, echo=FALSE, eval=TRUE}
dat <- read.csv("https://stats.idre.ucla.edu/stat/data/poisson_sim.csv")
dat <- within(dat, {
  prog <- factor(prog, 
                 levels=1:3, 
                 labels=c("General", "Academic", "Vocational"))
  id <- factor(id)
})
```

.pull-left[
.small[
```{r}
head(dat)
```
]
]

.pull-right[
.small[
```{r}
summary(dat)
```
]
]

Let's look at `math` as a predictor in a linear model.

---



```{r}
res1 <- lm(num_awards~math, data=dat)
summary(res1)
```

The fitted model is
$\widehat{y}_i = `r res1$coef[1]` + `r res1$coef[2]` \times \mbox{math}_i$
with both coefficients statistically different from 0 at the 5% level.


---

### Using linear regression with count data

.pull-left[

If $\mbox{math}_i = 0$ what goes wrong?

```{r, num_awards, fig.show="hide", echo=TRUE, eval=FALSE}
x    <- seq(0,max(dat$math),,1000)
yhat <- res1$coef[1] + res1$coef[2]*x
plot(dat$math,
     dat$num_awards,
     xlim=range(x),
     ylim=range(c(dat$num_awards,yhat)),
     xlab="score on final math exam",
     ylab="number of awards")
lines(x,yhat,col="red",lwd=3)
```

+ If we go below a math mark of about 30 we start predicting a negative number of awards.

+ In this context it might not matter much. 

+ However, in other contexts making positive predictions might be vital.


]

.pull-right[
```{r, ref.label="num_awards", echo=FALSE, eval=TRUE}
```
]

---

### Using linear regression with count data - Diagnostics

```{r, fig.width=15, fig.height=5}
par(mfrow = c(1, 4))
plot(res1)
```

The diagnostic plots look like a disaster with outliers, the tail distribution of the residuals 
reveal non-normally distribution of errors.

---

### From linear models to generalized linear models

For the Poisson distribution we had that

\begin{align}
P(Y = k) = {\displaystyle {\frac{\lambda^{k}e^{-\lambda}}{k!}}}
\end{align}

The parameter $\lambda>0$ controls the magnitude of the counts.

Suppose that we instead use

\begin{align}
\log(\lambda_i) & =\beta_0 + \beta_1 x_{i1} + \ldots + \beta_p x_{ip}
\end{align}

+ Now the mean of the Poisson distribution is guaranteed, through construction,
to have a positive mean!!!!

+ Because log of the parameter $\lambda$ is used, this is referred to as using the "log"-link.

+ Other links are possible, but the explanation is technical, and beyond the scope of this course.

---

### From linear models to generalized linear models

However, we can't use the R function `lm()` to fit this model any more.

Instead we use the R function `glm()`.

For comparison we used 

```{r, eval=FALSE}
res1 <- lm(num_awards~math, data=dat)
```

for linear models. For generalized linear models for Poisson data we use
almost identical syntax...

```{r, eval=FALSE}
res2 <- glm(num_awards~math, data=dat, family = poisson)
```

So we are fitting a model of the form

$$y_i \sim \mbox{Poisson}(\lambda_i) \quad \mbox{with} \qquad \lambda_i=\exp(\beta_0 + \beta_1\times \mbox{math}_i)$$



---

### From linear models to generalized linear models

.pull-left[




.small[
```{r}
res2 <- glm(num_awards~math, 
            data=dat, 
            family = poisson)
summary(res2)
```
]
]

.pull-right[

.small[
So the fitted model is

$$y_i \sim \mbox{Poisson}(\widehat{\lambda}_i)$$ 

with

$$\widehat{\lambda}_i=\exp(`r res2$coef[1]` + `r res2$coef[2]` \times \mbox{math}_i)$$

and both regression coefficients being statistically different to 0 at the 5 percent level.

]
]

---

### Plotting the results

.pull-left[

```{r, plot_poisson_fit, fig.show="hide", echo=TRUE, eval=FALSE}
x <- seq(0,max(dat$math),,1000)
df <- data.frame(math=x)
yhat <- predict(res2, 
                newdata=df, 
                type="response")

plot(dat$math,
     dat$num_awards,
     xlim=range(x),
     ylim=range(c(dat$num_awards,yhat)),
     xlab="score on final math exam",
     ylab="number of awards")
lines(x,yhat,col="red",lwd=3)
```
]

.pull-right[

```{r, ref.label="plot_poisson_fit", echo=FALSE, eval=TRUE}
```
]

 


---

### Interpretation - Poisson linear model

So the predicted mean of the fit was of the form

$$\widehat{\lambda}_i=\exp(`r res2$coef[1]` + `r res2$coef[2]` \times \mbox{math}_i)$$

We can interpret the coefficient $\widehat{\beta}_1 = `r res2$coef[2]`$

> If we increase the value of the predictor $x = \mbox{math}_i$ by 1 unit,
then the mean will increase by a **factor** of $\exp(`r res2$coef[2]`) = `r exp(res2$coef[2])`$.

---

### Poisson linear model with multiple predictors

We are going to now fit a slightly more complicated model where we use
both `math` and `prog` as predictors

The complication here is that `prog` is a categorical variable with 3 levels
("General", "Academic", "Vocational").

In R we use the syntax

```{r, eval=FALSE}
res3 <- glm(num_awards ~ math + prog, 
            data=dat, 
            family = poisson)
```


to fit this model. But what model are we actually fitting?


Note that
```{r}
levels(dat$prog)
```

---

### Poisson linear model with multiple predictors

Since `prog` is categorical with $K=3$ factors, R creates two dummy variables

+ $I(\mbox{prog}_i=\mbox{Academic})$ and

+ $I(\mbox{prog}_i=\mbox{Vocational})$.

where $I(\mbox{prog}_i=\mbox{General})$ is treated as the base category.

The fitted model

$$y_i \sim \mbox{Poisson}(\lambda_i)$$ 
with 

$$\lambda_i=\exp(\beta_0 + \beta_1 \mbox{math}_i + \beta_2 I(\mbox{prog}_i=\mbox{Academic}) + \beta_3 I(\mbox{prog}_i=\mbox{Vocational}))$$

Let's fit this model in R.

---

.small[
```{r}
res3 <- glm(num_awards~math + prog, data=dat, family = poisson)
summary(res3)
```
]


 
---

### The fitted model

The fitted model uses

$$\widehat{\lambda}_i=\exp(`r round(res3$coef[1],3)` + `r round(res3$coef[2],3)` \mbox{math}_i + `r round(res3$coef[3],3)` I(\mbox{prog}_i=\mbox{Academic}) + `r round(res3$coef[4],3)` I(\mbox{prog}_i=\mbox{Vocational}))$$

with

+ $\widehat{\beta}_0 = `r round(res3$coef[1],3)`$


+ $\widehat{\beta}_1 = `r round(res3$coef[2],3)`$

+ $\widehat{\beta}_2 = `r round(res3$coef[3],3)`$

+ $\widehat{\beta}_3 = `r round(res3$coef[4],3)`$

where $\beta_0$ (p-value $1.60\times10^{-15}$), $\beta_1$ (p-value $3.63\times 10^{-11}$), and 
$\beta_2$ (p-value 0.00248) being significantly different from 0 at the 5% level,
and $\beta_3$ (p-value 0.40179) not being significantly different from 0 at the 5% level.

---

### Prediction

We can make predictions if we have all of the covariates.

We need to enter all of the new covariates into a new data.frame
object and pass that to the predict function

```{r}
new_data = data.frame(math = 50, prog="Vocational")
predict(res3, newdata = new_data, type = "link")
```

---

### Interpreting the fitted coefficients

+ For every unit increase of $\mbox{math}_i$ the mean number of awards increases by a factor of 
$\exp(`r round(res3$coef[2],3)`) = `r round(exp(res3$coef[2]),3)`$.

+ If $\mbox{prog}_i=\mbox{Academic}$ the mean number of awards increases by a factor of 
$\exp(`r round(res3$coef[3],3)`) = `r round(exp(res3$coef[3]),3)`$ compared to if $I(\mbox{prog}_i=\mbox{General})$.


+ If $\mbox{prog}_i=\mbox{Vocational}$ the mean number of awards increases by a factor of 
$\exp(`r round(res3$coef[4],3)`) = `r round(exp(res3$coef[4]),3)`$ compared to if $I(\mbox{prog}_i=\mbox{General})$.

---

```{r, plot_poisson_fit2, fig.show="hide", echo=FALSE, eval=FALSE}
x <- seq(min(dat$math),max(dat$math),,1000)
dfG <- data.frame(math=x,prog="General")
dfA <- data.frame(math=x,prog="Academic")
dfV <- data.frame(math=x,prog="Vocational")
yhatG <- predict(res3, 
                 newdata=dfG, 
                 type="response")
yhatA <- predict(res3, 
                 newdata=dfA, 
                 type="response")
yhatV <- predict(res3, 
                 newdata=dfV, 
                 type="response")
plot(dat$math,
     dat$num_awards,
     type="n",
     xlim=range(x),
     ylim=range(dat$num_awards),
     xlab="score on final math exam",
     ylab="number of awards")
lines(x,yhatG,col="red",lwd=3)
lines(x,yhatA,col="blue",lwd=3)
lines(x,yhatV,col="green",lwd=3)

indsG <- which(dat$prog=="General")
indsA <- which(dat$prog=="Academic")
indsV <- which(dat$prog=="Vocational")

points(dat$math[indsG],dat$num_awards[indsG],col="red",pch=16)
points(dat$math[indsA],dat$num_awards[indsA],col="blue",pch=16)
points(dat$math[indsV],dat$num_awards[indsV],col="green",pch=16)

legend("topleft", 
       levels(dat$prog), 
       col=c("red", "blue", "green"), 
       lwd=3)
```


```{r, ref.label="plot_poisson_fit2", echo=FALSE, eval=TRUE, fig.width=12, fig.height=8}
```

---

### Model selection

There are two commonly used criterion for model selection. 

.pull-left[

The Akaike Information Criterion (AIC)
 
```{r}
AIC(res2)
AIC(res3)
```
]

.pull-right[
and the Bayesian Information Criterion (BIC)

```{r}
BIC(res2)
BIC(res3)
```

]


+ For both of these Criterion the lower value are better. 

+ So both would choose the
model with the `math` and `prog` as predictors as being the better model
compared to the model with only `math` as a predictor.

---

### Poisson regression variants

Other variants of regression for count data include:

+ Negative binomial regression: Introduces an additional parameter which can be used when the counts are 
overdispersed (mean < variance). This type of model can be fit using the glm.nb() function in
the MASS package or the manyglm() function in the mvabund package.

+ Zero inflated Poisson: Some count data contains an excessive number of zeroes. The package pscl
has the function zeroinfl() for dealing with this.


---
 
### Created using [R Markdown](https://rmarkdown.rstudio.com) with flair by [**xaringan**](https://github.com/yihui/xaringan)

<br> 

<a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.

  
